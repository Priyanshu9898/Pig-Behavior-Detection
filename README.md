# ğŸ· Pigâ€¯Behaviorâ€¯Detectionâ€¯System

Detect *eating, drinking, lying, moving,* and *standing* behaviors of pigs from videoâ€‘frame sequences recorded in commercial pens.

---

## ğŸ¯ Goals
| # | Objective |
|---|-----------|
| 1 | Build an endâ€‘toâ€‘end pipeline that ingests an MP4 sequence and produces perâ€‘frame behavior labels in the format expected by **`output.json`**. |
| 2 | Achieve accuracy comparable to the metrics reported in Tableâ€¯2 of the reference paperâ€”while **training is optional** for the grader. |
| 3 | Provide a clean, reproducible workflow that can be executed on a laptop, an onâ€‘prem GPU, or GoogleÂ Colab. |
| 4 | Summarize evaluation results in **`accuracy.txt`** exactly as required for submission. |

---

## ğŸ“š Dataset  
* **Source:** 12 annotated sequences in `annoted.tar` from <https://homepages.inf.ed.ac.uk/rbf/PIGDATA/>  
* **Train / Val split:**  
  * **Train:** first 7 subâ€‘folders - ['000002','000009','000016','000028','000036','000033','000010']
  * **Val:** last 5 subâ€‘folders - ['000113','000005','000208','000060','000078']
* **Ignore** any other unâ€‘annotated sequences on the site.

> **Tip:** Extract `annoted.tar` so the folder hierarchy looks like:
> ```
> data/
> â”œâ”€â”€ 01/
> â”‚   â”œâ”€â”€ color.mp4
> â”‚   â”œâ”€â”€ depth_scale.npy
> â”‚   â”œâ”€â”€ background.png
> â”‚   â””â”€â”€ output.json        # ground truth for this clip
> â””â”€â”€ ...
> ```

---

## ğŸ—ï¸ Project Structure
