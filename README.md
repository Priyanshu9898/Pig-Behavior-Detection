# ğŸ· Pigâ€¯Behaviorâ€¯Detectionâ€¯System

<p align="center">
  <a href="https://github.com/Priyanshu9898/Pig-Behavior-Detection/stargazers"><img src="https://img.shields.io/github/stars/Priyanshu9898/Pig-Behavior-Detection?style=for-the-badge" alt="Stars"></a>
  <a href="https://github.com/Priyanshu9898/Pig-Behavior-Detection/network/members"><img src="https://img.shields.io/github/forks/Priyanshu9898/Pig-Behavior-Detection?style=for-the-badge" alt="Forks"></a>
  <a href="https://github.com/Priyanshu9898/Pig-Behavior-Detection/issues"><img src="https://img.shields.io/github/issues/Priyanshu9898/Pig-Behavior-Detection?style=for-the-badge" alt="Issues"></a>
  <a href="https://github.com/Priyanshu9898/Pig-Behavior-Detection/blob/main/LICENSE"><img src="https://img.shields.io/github/license/Priyanshu9898/Pig-Behavior-Detection?style=for-the-badge" alt="License"></a>
</p>


Detect *eating, drinking, lying, moving,* and *standing* behaviors of pigs from videoâ€‘frame sequences recorded in commercial pens.

---

## ğŸ›  TechÂ Stack

| Layer | Technology | Badge |
|-------|------------|-------|
| Language | Pythonâ€¯3.10+ | ![Python](https://img.shields.io/badge/Python-3.10%2B-blue?logo=python&logoColor=white) |
| CV Model | UltralyticsÂ YOLOv8 | ![YOLOv8](https://img.shields.io/badge/YOLOv8-ultralytics-orange) |
| DL Framework | PyTorchÂ 2.2Â (+Â CUDAÂ 11.x) | ![PyTorch](https://img.shields.io/badge/PyTorch-2.2-EE4C2C?logo=pytorch&logoColor=white) |
| Libraries | OpenCVÂ 4.9, NumPy, TQDM | ![OpenCV](https://img.shields.io/badge/OpenCV-4.9-blueviolet?logo=opencv&logoColor=white) |


## ğŸ¯ Goals
| # | Objective |
|---|-----------|
| 1 | Build an endâ€‘toâ€‘end pipeline that ingests an MP4 sequence and produces perâ€‘frame behavior labels in the format expected by **`output.json`**. |
| 2 | Achieve accuracy comparable to the metrics reported in Tableâ€¯2 of the reference paperâ€”while **training is optional** for the grader. |
| 3 | Provide a clean, reproducible workflow that can be executed on a laptop, an onâ€‘prem GPU, or GoogleÂ Colab. |
| 4 | Summarize evaluation results in **`accuracy.txt`** exactly as required for submission. |

---

## ğŸ“š Dataset  
* **Source:** 12 annotated sequences in `annoted.tar` from <https://homepages.inf.ed.ac.uk/rbf/PIGDATA/>  
* **Train / Val split:**  
  * **Train:** first 7 subâ€‘folders - ['000002','000009','000016','000028','000036','000033','000010']
  * **Val:** last 5 subâ€‘folders - ['000113','000005','000208','000060','000078']
* **Ignore** any other unâ€‘annotated sequences on the site.

> **Tip:** Extract `annoted.tar` so the folder hierarchy looks like:
> ```
> data/
> â”œâ”€â”€ 01/
> â”‚   â”œâ”€â”€ color.mp4
> â”‚   â”œâ”€â”€ depth_scale.npy
> â”‚   â”œâ”€â”€ background.png
> â”‚   â””â”€â”€ output.json        # ground truth for this clip
> â””â”€â”€ ...
> ```

---

## ğŸš€ ProjectÂ Setup

> **Prerequisites**Â  â€‘Â PythonÂ â‰¥Â 3.10 â€¢Â Git â€¢Â (Optional) NVIDIA GPU w/Â CUDAÂ 11+

### 1Â Â·Â Clone the repository

```bash
git clone https://github.com/Priyanshu9898/Pig-Behavior-Detection.git
cd Pig-Behavior-Detection
```

### 2Â Â·Â Create & activate a virtual environment

| OS            | Command                                                      |
|---------------|--------------------------------------------------------------|
| macOS / Linux | `python -m venv env && source env/bin/activate`              |
| WindowsÂ PS    | `python -m venv env; .\env\Scripts\Activate.ps1`            |

### 3Â Â·Â Install dependencies

```bash
pip install -r requirements.txt             # runtime
```

### 4Â Â·Â Download & unpack the dataset

```bash
mkdir -p data
# Manually place annoted.tar inside data/ then
tar -xf data/annoted.tar -C data/
```

## ğŸ“ Notebook: `yolo_training.ipynb`

`yolo_training.ipynb` is for training yolo model.
1. **EnvironmentÂ Setup** â€“ Installs the required libraries and checks CUDA availability.
2. **DataÂ Prep** â€“ Extracts `annoted.tar`, applies the train/val split, and converts labels to YOLOv8 format.
3. **ModelÂ Config** â€“ Loads the YOLOv8â€‘small backbone.
4. **Training Loop** â€“ Trains for 50Â epochs.
5. **Checkpoint Export** â€“ Saves the best model to `weights/model.pt`.
6. **QuickÂ Validation** â€“ Runs inference on the validation set and prints `accuracy.txt`.

## ğŸ“ Notebook: `Complete_Pipeline.ipynb`
During inference, the code reads each video frame, applies YOLO every frame to detect pigs, tracks each detection over time with a lightweight MOSSE tracker, andâ€”on the flyâ€”classifies posture and uses simple motion and orientation heuristics to decide â€œmoving,â€ â€œeating,â€ or â€œdrinkingâ€ when appropriate. Finally, it assembles each pigâ€™s per-frame labels into the required JSON format, writes out an accuracy report comparing predictions to the groundâ€truth output.json files, and computes AP, TP, FP, and miss rates for each clip and overall.e.

## ğŸ” Detailed Pipeline Overview

### 1. FrameÂ CroppingÂ & ManifestÂ Creation
- Reads each clipâ€™s **`output.json`** to map annotated (everyâ€‘third) frames â†’ raw frames.
- Opens the color video, crops each pigâ€™s bounding box per frame, and saves crops into `data_crops/{train,val}/{behaviour}/`.
- Builds a **CSV manifest** (`manifest.csv`) listing every crop path and its label.

### 2. PostureÂ ModelÂ TrainingÂ *(ResNetâ€‘50)*
- Loads the manifest and filters to **lying** vs **standing** examples.
- Splits into train/val folds **by clip ID** to avoid leakage.
- Computes classâ€‘balanced **sampler** weights *and* **loss** weights.
- Applies heavy data augmentation: random crop, flip, rotation, colorâ€‘jitter, random erasing.
- Fineâ€‘tunes a **preâ€‘trained ResNetâ€‘50** (2â€‘class head) with mixed precision, cosine LR scheduler, and early stopping.
- Logs train/val loss & accuracy each epoch and saves the **best checkpoint**.

### 3. ObjectÂ DetectorÂ TrainingÂ *(YOLOv8)*
- Converts groundâ€‘truth boxes to YOLO TXT under `yolo/images/{train,val}` & `yolo/labels/{train,val}`.
- Writes `data.yaml` (1Â class: *pig*) and trains a YOLOv8 detector.
- Exports best `.pt` weights and optionally **ONNX** for deployment.

### 4. InferenceÂ PipelineÂ (per clip)
**a. PenÂ Mask Reconstruction** â€“ thresholds `background.png` to obtain the usableâ€‘pen binary mask.

**b. DetectionÂ & Tracking** â€“ runs YOLO on each resized frame; retains detections whose centroids fall **inside** the mask and maintains **MOSSE trackers** to bridge gaps.

**c. Onâ€‘theâ€‘fly Behavior Classification**  
â€¢ **Moving:** compares centroid displacement overÂ 2â€¯s (via `depth_scale`) against a cm threshold.  
â€¢ **Eating / Drinking:** uses imageâ€‘moment orientation to check alignment toward feeder vs waterer coordinates.  
â€¢ **Lying / Standing:** batches crops through the ResNet posture model for posture prediction.

**d. JSONÂ Export** â€“ collates each trackletâ€™s perâ€‘frame behavior changes into the required **`output.json`** format.

### 5. EvaluationÂ & Reporting
- Compares each predicted JSON against ground truth per clip.
- Computes **AP**, *trueâ€‘positiveâ€¯%*, *falseâ€‘positiveâ€¯%*, and *missedâ€‘detectionâ€¯%* for each sequence and overall.
- Writes an **`accuracy.txt`** summary matching TableÂ 2 of the paper.

---

## ğŸ¤ Contributing & License

PRs and issues are welcome! Code is released under the MIT License â€“ see `LICENSE`.  

---
